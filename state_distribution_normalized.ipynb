{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwi9XFXMVKiv",
        "outputId": "3e7b1068-5606-42f0-9d96-4ab441264f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_discrete: True\n",
            "----------------------------------------\n",
            "| Iteration 0\n",
            "| Actor Loss: -0.698, Critic Loss: 0.291, Alpha: 0.9997\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 1000\n",
            "| Actor Loss: -3.265, Critic Loss: 0.040, Alpha: 0.8976\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 2000\n",
            "| Actor Loss: -5.881, Critic Loss: 0.037, Alpha: 0.7990\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 3000\n",
            "| Actor Loss: -8.242, Critic Loss: 0.028, Alpha: 0.5756\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 4000\n",
            "| Actor Loss: -9.757, Critic Loss: 0.034, Alpha: 0.3270\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 5000\n",
            "| Actor Loss: -10.680, Critic Loss: 0.045, Alpha: 0.1682\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 5000 | Avg Reward: 26.60 ± 23.31 | Avg Discounted Return: 0.03 ---\n",
            "----------------------------------------\n",
            "| Iteration 6000\n",
            "| Actor Loss: -11.186, Critic Loss: 0.043, Alpha: 0.0896\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 7000\n",
            "| Actor Loss: -11.467, Critic Loss: 0.034, Alpha: 0.0490\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 8000\n",
            "| Actor Loss: -11.858, Critic Loss: 0.057, Alpha: 0.0280\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 9000\n",
            "| Actor Loss: -12.005, Critic Loss: 0.073, Alpha: 0.0171\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 10000\n",
            "| Actor Loss: -12.272, Critic Loss: 0.049, Alpha: 0.0112\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 10000 | Avg Reward: 37.60 ± 10.36 | Avg Discounted Return: 0.05 ---\n",
            "----------------------------------------\n",
            "| Iteration 11000\n",
            "| Actor Loss: -12.363, Critic Loss: 0.067, Alpha: 0.0077\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 12000\n",
            "| Actor Loss: -12.594, Critic Loss: 0.101, Alpha: 0.0055\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 13000\n",
            "| Actor Loss: -12.745, Critic Loss: 0.062, Alpha: 0.0040\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 14000\n",
            "| Actor Loss: -12.932, Critic Loss: 0.086, Alpha: 0.0029\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 15000\n",
            "| Actor Loss: -13.069, Critic Loss: 0.085, Alpha: 0.0021\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 15000 | Avg Reward: 38.20 ± 24.12 | Avg Discounted Return: 0.08 ---\n",
            "----------------------------------------\n",
            "| Iteration 16000\n",
            "| Actor Loss: -13.110, Critic Loss: 0.073, Alpha: 0.0016\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 17000\n",
            "| Actor Loss: -13.078, Critic Loss: 0.110, Alpha: 0.0012\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 18000\n",
            "| Actor Loss: -13.046, Critic Loss: 0.088, Alpha: 0.0009\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 19000\n",
            "| Actor Loss: -13.277, Critic Loss: 0.139, Alpha: 0.0006\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 20000\n",
            "| Actor Loss: -13.191, Critic Loss: 0.082, Alpha: 0.0005\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 20000 | Avg Reward: 43.90 ± 26.27 | Avg Discounted Return: 0.08 ---\n",
            "----------------------------------------\n",
            "| Iteration 21000\n",
            "| Actor Loss: -12.624, Critic Loss: 0.080, Alpha: 0.0003\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 22000\n",
            "| Actor Loss: -12.951, Critic Loss: 0.109, Alpha: 0.0003\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 23000\n",
            "| Actor Loss: -12.610, Critic Loss: 0.098, Alpha: 0.0002\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 24000\n",
            "| Actor Loss: -12.870, Critic Loss: 0.082, Alpha: 0.0001\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 25000\n",
            "| Actor Loss: -13.266, Critic Loss: 0.146, Alpha: 0.0001\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 25000 | Avg Reward: 176.40 ± 188.64 | Avg Discounted Return: 0.21 ---\n",
            "----------------------------------------\n",
            "| Iteration 26000\n",
            "| Actor Loss: -12.716, Critic Loss: 0.135, Alpha: 0.0001\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 27000\n",
            "| Actor Loss: -12.963, Critic Loss: 0.127, Alpha: 0.0001\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 28000\n",
            "| Actor Loss: -12.969, Critic Loss: 0.148, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 29000\n",
            "| Actor Loss: -12.344, Critic Loss: 0.158, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 30000\n",
            "| Actor Loss: -12.878, Critic Loss: 0.103, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 30000 | Avg Reward: 93.20 ± 99.20 | Avg Discounted Return: 0.12 ---\n",
            "----------------------------------------\n",
            "| Iteration 31000\n",
            "| Actor Loss: -13.279, Critic Loss: 0.198, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 32000\n",
            "| Actor Loss: -12.233, Critic Loss: 0.155, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 33000\n",
            "| Actor Loss: -12.622, Critic Loss: 0.127, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 34000\n",
            "| Actor Loss: -12.878, Critic Loss: 0.116, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 35000\n",
            "| Actor Loss: -13.321, Critic Loss: 0.124, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 35000 | Avg Reward: 126.90 ± 146.35 | Avg Discounted Return: 0.10 ---\n",
            "----------------------------------------\n",
            "| Iteration 36000\n",
            "| Actor Loss: -13.361, Critic Loss: 0.151, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 37000\n",
            "| Actor Loss: -12.791, Critic Loss: 0.158, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 38000\n",
            "| Actor Loss: -13.824, Critic Loss: 0.131, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 39000\n",
            "| Actor Loss: -14.398, Critic Loss: 0.163, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 40000\n",
            "| Actor Loss: -14.307, Critic Loss: 0.165, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 40000 | Avg Reward: 165.10 ± 181.27 | Avg Discounted Return: 0.16 ---\n",
            "----------------------------------------\n",
            "| Iteration 41000\n",
            "| Actor Loss: -13.636, Critic Loss: 0.136, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 42000\n",
            "| Actor Loss: -14.396, Critic Loss: 0.188, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 43000\n",
            "| Actor Loss: -14.682, Critic Loss: 0.132, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 44000\n",
            "| Actor Loss: -14.317, Critic Loss: 0.175, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 45000\n",
            "| Actor Loss: -14.687, Critic Loss: 0.187, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 45000 | Avg Reward: 110.20 ± 107.05 | Avg Discounted Return: 0.09 ---\n",
            "----------------------------------------\n",
            "| Iteration 46000\n",
            "| Actor Loss: -14.148, Critic Loss: 0.176, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 47000\n",
            "| Actor Loss: -14.482, Critic Loss: 0.154, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 48000\n",
            "| Actor Loss: -15.418, Critic Loss: 0.194, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 49000\n",
            "| Actor Loss: -15.391, Critic Loss: 0.177, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 50000\n",
            "| Actor Loss: -15.355, Critic Loss: 0.191, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 50000 | Avg Reward: 223.70 ± 178.73 | Avg Discounted Return: 0.16 ---\n",
            "----------------------------------------\n",
            "| Iteration 51000\n",
            "| Actor Loss: -15.574, Critic Loss: 0.254, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 52000\n",
            "| Actor Loss: -15.504, Critic Loss: 0.191, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 53000\n",
            "| Actor Loss: -16.028, Critic Loss: 0.222, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 54000\n",
            "| Actor Loss: -15.584, Critic Loss: 0.167, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 55000\n",
            "| Actor Loss: -15.998, Critic Loss: 0.268, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 55000 | Avg Reward: 241.90 ± 178.21 | Avg Discounted Return: 0.18 ---\n",
            "----------------------------------------\n",
            "| Iteration 56000\n",
            "| Actor Loss: -15.948, Critic Loss: 0.181, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 57000\n",
            "| Actor Loss: -15.953, Critic Loss: 0.181, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 58000\n",
            "| Actor Loss: -15.954, Critic Loss: 0.211, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 59000\n",
            "| Actor Loss: -16.346, Critic Loss: 0.207, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 60000\n",
            "| Actor Loss: -15.781, Critic Loss: 0.207, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 60000 | Avg Reward: 280.20 ± 187.40 | Avg Discounted Return: 0.25 ---\n",
            "----------------------------------------\n",
            "| Iteration 61000\n",
            "| Actor Loss: -16.512, Critic Loss: 0.246, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 62000\n",
            "| Actor Loss: -16.058, Critic Loss: 0.227, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 63000\n",
            "| Actor Loss: -16.255, Critic Loss: 0.239, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 64000\n",
            "| Actor Loss: -16.214, Critic Loss: 0.261, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 65000\n",
            "| Actor Loss: -16.226, Critic Loss: 0.268, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 65000 | Avg Reward: 193.40 ± 182.35 | Avg Discounted Return: 0.17 ---\n",
            "----------------------------------------\n",
            "| Iteration 66000\n",
            "| Actor Loss: -16.527, Critic Loss: 0.195, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 67000\n",
            "| Actor Loss: -16.705, Critic Loss: 0.233, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 68000\n",
            "| Actor Loss: -16.261, Critic Loss: 0.213, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 69000\n",
            "| Actor Loss: -16.975, Critic Loss: 0.223, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 70000\n",
            "| Actor Loss: -16.029, Critic Loss: 0.240, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 70000 | Avg Reward: 229.20 ± 190.79 | Avg Discounted Return: 0.19 ---\n",
            "----------------------------------------\n",
            "| Iteration 71000\n",
            "| Actor Loss: -16.324, Critic Loss: 0.249, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 72000\n",
            "| Actor Loss: -16.619, Critic Loss: 0.239, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 73000\n",
            "| Actor Loss: -17.218, Critic Loss: 0.227, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 74000\n",
            "| Actor Loss: -16.569, Critic Loss: 0.193, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 75000\n",
            "| Actor Loss: -16.880, Critic Loss: 0.246, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 75000 | Avg Reward: 183.00 ± 124.04 | Avg Discounted Return: 0.13 ---\n",
            "----------------------------------------\n",
            "| Iteration 76000\n",
            "| Actor Loss: -17.108, Critic Loss: 0.223, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 77000\n",
            "| Actor Loss: -16.580, Critic Loss: 0.255, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 78000\n",
            "| Actor Loss: -17.255, Critic Loss: 0.230, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 79000\n",
            "| Actor Loss: -17.049, Critic Loss: 0.269, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 80000\n",
            "| Actor Loss: -17.408, Critic Loss: 0.282, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 80000 | Avg Reward: 166.90 ± 147.39 | Avg Discounted Return: 0.10 ---\n",
            "----------------------------------------\n",
            "| Iteration 81000\n",
            "| Actor Loss: -17.311, Critic Loss: 0.248, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 82000\n",
            "| Actor Loss: -17.713, Critic Loss: 0.211, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 83000\n",
            "| Actor Loss: -16.823, Critic Loss: 0.246, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 84000\n",
            "| Actor Loss: -17.545, Critic Loss: 0.275, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 85000\n",
            "| Actor Loss: -16.919, Critic Loss: 0.305, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 85000 | Avg Reward: 140.70 ± 153.73 | Avg Discounted Return: 0.11 ---\n",
            "----------------------------------------\n",
            "| Iteration 86000\n",
            "| Actor Loss: -17.505, Critic Loss: 0.264, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 87000\n",
            "| Actor Loss: -18.491, Critic Loss: 0.245, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 88000\n",
            "| Actor Loss: -18.344, Critic Loss: 0.235, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 89000\n",
            "| Actor Loss: -18.035, Critic Loss: 0.265, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 90000\n",
            "| Actor Loss: -18.598, Critic Loss: 0.237, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 90000 | Avg Reward: 128.80 ± 128.53 | Avg Discounted Return: 0.09 ---\n",
            "----------------------------------------\n",
            "| Iteration 91000\n",
            "| Actor Loss: -18.565, Critic Loss: 0.298, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 92000\n",
            "| Actor Loss: -18.695, Critic Loss: 0.237, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 93000\n",
            "| Actor Loss: -18.320, Critic Loss: 0.302, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 94000\n",
            "| Actor Loss: -18.137, Critic Loss: 0.294, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 95000\n",
            "| Actor Loss: -18.377, Critic Loss: 0.260, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "--- Evaluation | Iteration 95000 | Avg Reward: 234.80 ± 158.63 | Avg Discounted Return: 0.14 ---\n",
            "----------------------------------------\n",
            "| Iteration 96000\n",
            "| Actor Loss: -18.705, Critic Loss: 0.290, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 97000\n",
            "| Actor Loss: -18.328, Critic Loss: 0.405, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 98000\n",
            "| Actor Loss: -17.905, Critic Loss: 0.292, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| Iteration 99000\n",
            "| Actor Loss: -18.672, Critic Loss: 0.302, Alpha: 0.0000\n",
            "----------------------------------------\n",
            "Saving models to checkpoints/url_checkpoint_MultiValleyMountainCarDiscrete-v0_final\n",
            "CSV logs saved to logs/SAC_MultiValleyMountainCarDiscrete-v0_20251118_132850_metrics.csv\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict, deque\n",
        "from torch.distributions import Categorical\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "from matplotlib import animation\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Setup\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "torch.set_default_dtype(torch.float32)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "rng = np.random.default_rng(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Environment\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "min_position = -0.99\n",
        "max_position =  0.99\n",
        "max_speed    =  0.07\n",
        "dt           =  0.1\n",
        "force_mag    =  0.001\n",
        "gravity      =  0.0025\n",
        "\n",
        "min_start_pos = 0.67\n",
        "max_start_pos = 0.77\n",
        "max_start_vel = 0.01\n",
        "\n",
        "state_low  = torch.tensor([min_position, -max_speed], device=device)\n",
        "state_high = torch.tensor([max_position,  max_speed], device=device)\n",
        "state_mid  = 0.5 * (state_high + state_low)\n",
        "state_span = 0.5 * (state_high - state_low)\n",
        "\n",
        "def normalize_state(states: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Scale (pos, vel) to [-1, 1] for the RFF layer.\"\"\"\n",
        "    return (states - state_mid) / state_span\n",
        "\n",
        "box_area   = (max_position - min_position) * (2 * max_speed)\n",
        "\n",
        "\n",
        "def tg_alpha(x: torch.Tensor) -> torch.Tensor:\n",
        "    return 0.1 * (\n",
        "        2 * x / (1 - x**2)\n",
        "        - 2 * math.pi * torch.sin(2 * math.pi * x)\n",
        "        - 8 * math.pi * torch.sin(4 * math.pi * x)\n",
        "    )\n",
        "\n",
        "\n",
        "def total_horizontal_force(a: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    # a in {0,1}\n",
        "    return (a * 2 - 1) * force_mag + tg_alpha(x) * (-gravity)\n",
        "\n",
        "\n",
        "def next_state(s: torch.Tensor, a: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    s: (N,2) with (x,v)\n",
        "    a: (N,) 0 or 1\n",
        "    \"\"\"\n",
        "    x = s[:, 0]\n",
        "    v = s[:, 1]\n",
        "    f = total_horizontal_force(a.float(), x)\n",
        "\n",
        "    v = torch.clamp(v + f * dt, -max_speed, max_speed)\n",
        "    x = torch.clamp(x + v * dt, min_position, max_position)\n",
        "\n",
        "    return torch.stack([x, v], dim=1)\n",
        "\n",
        "\n",
        "def sample_proposal(n: int) -> torch.Tensor:\n",
        "    \"\"\"Uniform proposal over the state box.\"\"\"\n",
        "    u = torch.rand(n, 2, device=device)\n",
        "    s = state_low + u * (state_high - state_low)\n",
        "    return s\n",
        "\n",
        "\n",
        "def sample_start(n: int) -> torch.Tensor:\n",
        "    \"\"\"Initial distribution d0.\"\"\"\n",
        "    sign = torch.where(\n",
        "        torch.rand(n, device=device) < 0.5,\n",
        "        -torch.ones(n, device=device),\n",
        "        torch.ones(n, device=device)\n",
        "    )\n",
        "    pos = sign * torch.rand(n, device=device) * (max_start_pos - min_start_pos) + sign * min_start_pos\n",
        "    vel = (torch.rand(n, device=device) * 2 - 1) * max_start_vel\n",
        "\n",
        "    return torch.stack([pos, vel], dim=1)\n",
        "\n",
        "\n",
        "def render_state_distribution_video(frames, extent, norm, interval_ms=200, save_path=None):\n",
        "    \"\"\"Render an HTML5 video showing how the density evolves over time.\"\"\"\n",
        "    if not frames:\n",
        "        return\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.set_xlabel('Position')\n",
        "    ax.set_ylabel('Velocity')\n",
        "    im = ax.imshow(\n",
        "        frames[0][1].T,\n",
        "        extent=extent,\n",
        "        origin='lower',\n",
        "        cmap='gray',\n",
        "        aspect='auto',\n",
        "        norm=norm\n",
        "    )\n",
        "    title = ax.set_title(f'Learned Distribution (Iteration {frames[0][0]})')\n",
        "\n",
        "    def update(frame):\n",
        "        iteration, density_values = frame\n",
        "        im.set_data(density_values.T)\n",
        "        title.set_text(f'Learned Distribution (Iteration {iteration})')\n",
        "        return im,\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig,\n",
        "        update,\n",
        "        frames=frames,\n",
        "        interval=interval_ms,\n",
        "        repeat=False\n",
        "    )\n",
        "    fps = max(1, int(1000 / interval_ms)) if interval_ms > 0 else 5\n",
        "    if save_path:\n",
        "        target_path = str(save_path)\n",
        "        if not target_path.lower().endswith('.gif'):\n",
        "            target_path = f\"{target_path}.gif\"\n",
        "        writer = animation.PillowWriter(fps=fps)\n",
        "        anim.save(target_path, writer=writer)\n",
        "    html = HTML(anim.to_jshtml())\n",
        "    plt.close(fig)\n",
        "    display(html)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Fixed policy\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._fc_policy = nn.Sequential(\n",
        "            nn.Linear(2, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._fc_policy(x)\n",
        "\n",
        "\n",
        "policy_net = PolicyNetwork().to(device)\n",
        "\n",
        "# >>> EDIT THIS PATH IF NEEDED <<<\n",
        "state_dict = torch.load(\n",
        "    r\"/content/net.pth\",\n",
        "    map_location=device\n",
        ")\n",
        "policy_state = OrderedDict(\n",
        "    {k.replace('_fc_policy.', ''): v\n",
        "     for k, v in state_dict.items()\n",
        "     if k.startswith('_fc_policy')}\n",
        ")\n",
        "policy_net._fc_policy.load_state_dict(policy_state)\n",
        "policy_net.eval()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_actions(s: torch.Tensor) -> torch.Tensor:\n",
        "    logits = policy_net(s)\n",
        "    dist = Categorical(logits=logits)\n",
        "    return dist.sample()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def policy_probs(states: torch.Tensor) -> torch.Tensor:\n",
        "    logits = policy_net(states)\n",
        "    return F.softmax(logits, dim=-1)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Density network\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, d: int, use_ln: bool = False, alpha: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.ln1 = nn.LayerNorm(d) if use_ln else nn.Identity()\n",
        "        self.ln2 = nn.LayerNorm(d) if use_ln else nn.Identity()\n",
        "        self.act = nn.SiLU()\n",
        "        self.fc1 = nn.Linear(d, d)\n",
        "        self.fc2 = nn.Linear(d, d)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.act(self.ln1(x))\n",
        "        y = self.fc1(y)\n",
        "        y = self.act(self.ln2(y))\n",
        "        y = self.fc2(y)\n",
        "        return x + self.alpha * y\n",
        "\n",
        "\n",
        "class SiLUDensityRes(nn.Module):\n",
        "    \"\"\"\n",
        "    Unnormalized density with residual MLP, plus a global logZ normalizer.\n",
        "    log_prob(s) = f_theta(s) - logZ\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden: int = 256, blocks: int = 5,\n",
        "                 use_ln: bool = True, box_area: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.inp = nn.Linear(2, hidden)\n",
        "        nn.init.kaiming_normal_(self.inp.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.inp.bias)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            ResidualBlock(hidden, use_ln=use_ln, alpha=0.1)\n",
        "            for _ in range(blocks)\n",
        "        ])\n",
        "\n",
        "        self.out = nn.Linear(hidden, 1)\n",
        "        nn.init.kaiming_normal_(self.out.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.out.bias)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.out.bias.fill_(-math.log(box_area))\n",
        "\n",
        "        self.register_buffer(\"logZ\", torch.zeros(1))\n",
        "\n",
        "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.inp(x)\n",
        "        h = self.blocks(h)\n",
        "        return self.out(h).squeeze(-1) - self.logZ\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.log_prob(x)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ---------------------------------------------------------------------\n",
        "# Median heuristic for RFF frequencies\n",
        "with torch.no_grad():\n",
        "    median_sample = sample_proposal(4096)\n",
        "    median_sample_norm = normalize_state(median_sample)\n",
        "    pairwise_distances = torch.pdist(median_sample_norm, p=2)\n",
        "    median_distance = pairwise_distances.median().item()\n",
        "\n",
        "\n",
        "heuristic_multiplier = 4.0\n",
        "rff_sigma_base = 1.0 / max(median_distance, 1e-6)  * heuristic_multiplier\n",
        "rff_sigma_low = 0.1 * rff_sigma_base\n",
        "rff_sigma_mid = 1.0 * rff_sigma_base\n",
        "rff_sigma_high = 40.0 * rff_sigma_base\n",
        "\n",
        "print(f\"Median normalized distance: {median_distance:.6f}\")\n",
        "print(f\"RFF sigma base set to {rff_sigma_base:.6f}\")\n",
        "print(f\"Low/Mid/High sigmas: {rff_sigma_low:.6f}, {rff_sigma_mid:.6f}, {rff_sigma_high:.6f}\")\n",
        "\n",
        "#  RFF sampler (3 bands with your hyperparameters)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "class RFFSampler2D:\n",
        "    def __init__(self,\n",
        "                 sigma_base: float = None,\n",
        "                 device=None,\n",
        "                 dtype=torch.float32):\n",
        "        self.device = device or torch.device('cpu')\n",
        "        self.dtype = dtype\n",
        "\n",
        "        if sigma_base is None:\n",
        "            if 'rff_sigma_base' not in globals():\n",
        "                raise ValueError(\"Compute rff_sigma_base via the median heuristic cell first.\")\n",
        "            sigma_base = float(rff_sigma_base)\n",
        "\n",
        "        self._set_sigmas(sigma_base)\n",
        "\n",
        "        print(\"Initialized RFF Sampler with equal-probability frequency bands:\")\n",
        "        print(f\"  LOW : sigma={self.sigma_low.item():.4f}\")\n",
        "        print(f\"  MID : sigma={self.sigma_mid.item():.4f}\")\n",
        "        print(f\"  HIGH: sigma={self.sigma_high.item():.4f}\")\n",
        "\n",
        "        self.omegas = None\n",
        "        self.biases = None\n",
        "\n",
        "    def _set_sigmas(self, sigma_base: float):\n",
        "        sigma_tensor = torch.tensor(sigma_base, device=self.device, dtype=self.dtype)\n",
        "        self.sigma_base = sigma_tensor\n",
        "        self.sigma_low = 0.1 * sigma_tensor\n",
        "        self.sigma_mid = sigma_tensor\n",
        "        self.sigma_high = 10.0 * sigma_tensor\n",
        "\n",
        "    def update_sigma_base(self, sigma_base: float):\n",
        "        self._set_sigmas(sigma_base)\n",
        "        # clear cached banks so fresh frequencies reflect the new scale\n",
        "        self.omegas = None\n",
        "        self.biases = None\n",
        "        print(f\"[RFF] Updated sigma base -> {self.sigma_base.item():.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _band(self, sigma, K, device, dtype):\n",
        "        return torch.normal(0.0, sigma, (K, 2), device=device, dtype=dtype)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_batch(self, B: int, device=None, dtype=None):\n",
        "        \"\"\"\n",
        "        Sample B RFFs using an equal-probability mixture of low/mid/high frequencies.\n",
        "        \"\"\"\n",
        "        device = device or self.device\n",
        "        dtype = dtype or self.dtype\n",
        "\n",
        "        B1 = B // 3\n",
        "        B2 = B // 3\n",
        "        B3 = B - B1 - B2\n",
        "\n",
        "        O1 = self._band(self.sigma_low,  B1, device, dtype)\n",
        "        O2 = self._band(self.sigma_mid,  B2, device, dtype)\n",
        "        O3 = self._band(self.sigma_high, B3, device, dtype)\n",
        "\n",
        "        self.omegas = torch.cat([O1, O2, O3], dim=0)            # (B,2)\n",
        "        self.biases = 2 * torch.pi * torch.rand(B, device=device, dtype=dtype)\n",
        "        return self.omegas, self.biases\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def features(self, states: torch.Tensor,\n",
        "                 omegas: torch.Tensor = None,\n",
        "                 biases: torch.Tensor = None) -> torch.Tensor:\n",
        "        if omegas is None or biases is None:\n",
        "            if self.omegas is None or self.biases is None:\n",
        "                raise ValueError(\"Call sample_batch(B) first or supply omegas/biases.\")\n",
        "            omegas, biases = self.omegas, self.biases\n",
        "\n",
        "        states_norm = normalize_state(states)\n",
        "        proj = states_norm @ omegas.T\n",
        "        return torch.cos(proj + biases)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Sigma refresh helper\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_sigma_from_density(density: nn.Module,\n",
        "                                sample_size: int = 4096,\n",
        "                                pool_multiplier: int = 4) -> tuple[float, float]:\n",
        "    total = sample_size * max(1, pool_multiplier)\n",
        "    pool_states = sample_proposal(total)\n",
        "    logw = density.log_prob(pool_states)\n",
        "    logw = logw - logw.max()\n",
        "    weights = torch.exp(logw)\n",
        "    probs = weights / weights.sum()\n",
        "    idx = torch.multinomial(probs, num_samples=sample_size, replacement=True)\n",
        "    selected = pool_states[idx]\n",
        "    normalized = normalize_state(selected)\n",
        "    distances = torch.pdist(normalized, p=2)\n",
        "    median_distance = distances.median().item()\n",
        "    sigma = 1.0 / max(median_distance, 1e-6)\n",
        "    return sigma, median_distance\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  U-gradient estimator\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def rff_expect_next_feature(states: torch.Tensor,\n",
        "                            probs: torch.Tensor,\n",
        "                            sampler: RFFSampler2D,\n",
        "                            omega: torch.Tensor,\n",
        "                            b: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    E_a[ f(s⁺) ] under policy.\n",
        "    \"\"\"\n",
        "    N = states.size(0)\n",
        "    a0 = torch.zeros(N, dtype=torch.long, device=states.device)\n",
        "    a1 = torch.ones(N,  dtype=torch.long, device=states.device)\n",
        "\n",
        "    sp0 = next_state(states, a0)\n",
        "    sp1 = next_state(states, a1)\n",
        "\n",
        "    F_sp0 = sampler.features(sp0, omega, b)\n",
        "    F_sp1 = sampler.features(sp1, omega, b)\n",
        "\n",
        "    return probs[:, 0:1] * F_sp0 + probs[:, 1:2] * F_sp1\n",
        "\n",
        "\n",
        "def estimate_R_and_grad(N: int,\n",
        "                        gamma: float,\n",
        "                        sampler: RFFSampler2D,\n",
        "                        omega: torch.Tensor,\n",
        "                        b: torch.Tensor,\n",
        "                        density: nn.Module,\n",
        "                        box_area: float):\n",
        "    \"\"\"\n",
        "    Compute R_f and its gradient factor via U-gradient.\n",
        "    \"\"\"\n",
        "    # ----- Batch S for R_f -----\n",
        "    S  = sample_proposal(N)\n",
        "    S0 = sample_start(N)\n",
        "    pS = policy_probs(S)\n",
        "\n",
        "    F_S   = sampler.features(S,  omega, b)        # (N,B)\n",
        "    EF_Sp = rff_expect_next_feature(S, pS, sampler, omega, b)  # (N,B)\n",
        "    F_S0  = sampler.features(S0, omega, b)        # (N,B)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logw_S = density.log_prob(S) + math.log(box_area)\n",
        "        w_S = torch.exp(logw_S)\n",
        "\n",
        "        R_all = (w_S.unsqueeze(1) * (F_S - gamma * EF_Sp)).mean(dim=0) \\\n",
        "                - (1.0 - gamma) * F_S0.mean(dim=0)\n",
        "        R_all = R_all.detach()\n",
        "\n",
        "    # ----- Independent St for grad -----\n",
        "    St  = sample_proposal(N)\n",
        "    pSt = policy_probs(St)\n",
        "\n",
        "    F_St   = sampler.features(St,  omega, b)\n",
        "    EF_Stp = rff_expect_next_feature(St, pSt, sampler, omega, b)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        A_f = (F_St - gamma * EF_Stp).detach()\n",
        "\n",
        "    logwt = density.log_prob(St) + math.log(box_area)\n",
        "    wt = torch.exp(logwt)\n",
        "\n",
        "    grad_all = (A_f * wt.unsqueeze(1)).mean(dim=0)\n",
        "\n",
        "    return R_all, grad_all\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Train step (fixed RFF bank)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def train_step(N: int,\n",
        "               gamma: float,\n",
        "               sampler: RFFSampler2D,\n",
        "               density: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               box_area: float,\n",
        "               B: int):\n",
        "\n",
        "    omega, b = sampler.sample_batch(B, device=device, dtype=torch.float32)\n",
        "\n",
        "    R_all, grad_all = estimate_R_and_grad(\n",
        "        N=N, gamma=gamma, sampler=sampler,\n",
        "        omega=omega, b=b,\n",
        "        density=density, box_area=box_area\n",
        "    )\n",
        "\n",
        "    loss = 2.0 * (R_all * grad_all).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(density.parameters(), max_norm=5.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "#  Training loop\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def train(T=10000, N=15000,\n",
        "          B=1024,\n",
        "          gamma=0.99, lr=1e-3,\n",
        "          print_every=50, visualize_every=200,\n",
        "          logZ_tau=0.1,\n",
        "          make_video=True,\n",
        "          max_video_frames=200,\n",
        "          video_path=None,\n",
        "          sigma_update_every=500,\n",
        "          sigma_sample_size=4096):\n",
        "\n",
        "    # visualization grid\n",
        "    pos_grid = torch.linspace(min_position, max_position, 400, device=device)\n",
        "    vel_grid = torch.linspace(-max_speed,  max_speed,  400, device=device)\n",
        "    P, V = torch.meshgrid(pos_grid, vel_grid, indexing='ij')\n",
        "    grid_states = torch.stack((P.flatten(), V.flatten()), dim=1)\n",
        "    plot_extent = [min_position, max_position, -max_speed, max_speed]\n",
        "\n",
        "    # CSV logging\n",
        "    csv_filename = 'training_metrics.csv'\n",
        "    csv_file = open(csv_filename, 'w', newline='')\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['Iteration', 'Loss', 'MeanWeight', 'BowlWeight', 'BridgeWeight'])\n",
        "\n",
        "    # modules\n",
        "    density = SiLUDensityRes(hidden=256, blocks=5, use_ln=True, box_area=box_area).to(device)\n",
        "    optimizer = torch.optim.Adam(density.parameters(), lr=lr)\n",
        "    sampler = RFFSampler2D(device=device, sigma_base=float(rff_sigma_base))\n",
        "    plot_norm = colors.PowerNorm(gamma=0.4, vmin=0.0, vmax=100.0)\n",
        "    video_frames = deque(maxlen=max_video_frames if max_video_frames else None) if make_video else None\n",
        "\n",
        "    hist = []\n",
        "    for t in range(1, T + 1):\n",
        "        L_val = train_step(\n",
        "            N=N, gamma=gamma,\n",
        "            sampler=sampler, density=density,\n",
        "            optimizer=optimizer, box_area=box_area,\n",
        "            B=B\n",
        "        )\n",
        "        hist.append(L_val)\n",
        "\n",
        "        if sigma_update_every and t % sigma_update_every == 0:\n",
        "            new_sigma, median_dist = estimate_sigma_from_density(\n",
        "                density,\n",
        "                sample_size=sigma_sample_size\n",
        "            )\n",
        "            sampler.update_sigma_base(new_sigma)\n",
        "            globals()['rff_sigma_base'] = new_sigma\n",
        "            globals()['rff_sigma_low'] = 0.1 * new_sigma\n",
        "            globals()['rff_sigma_mid'] = new_sigma\n",
        "            globals()['rff_sigma_high'] = 10.0 * new_sigma\n",
        "            print(\n",
        "                f\"[sigma] Iter {t}: base={new_sigma:.6f} (median_dist={median_dist:.6f})\"\n",
        "            )\n",
        "\n",
        "        # visualization\n",
        "        if t % visualize_every == 0:\n",
        "            clear_output(wait=True)\n",
        "            with torch.no_grad():\n",
        "                logd = density.log_prob(grid_states)\n",
        "                d_values = torch.exp(logd).cpu().numpy().reshape(400, 400)\n",
        "            if video_frames is not None:\n",
        "                video_frames.append((t, d_values.copy()))\n",
        "            print(f\"Iteration {t}/{T}: L_est={L_val:.6f}\")\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            im = ax.imshow(\n",
        "                d_values.T,\n",
        "                extent=plot_extent,\n",
        "                origin='lower',\n",
        "                cmap='gray',\n",
        "                aspect='auto',\n",
        "                norm=plot_norm\n",
        "            )\n",
        "            plt.colorbar(im, ax=ax)\n",
        "            ax.set_xlabel('Position')\n",
        "            ax.set_ylabel('Velocity')\n",
        "            ax.set_title(f'Learned Distribution (Iteration {t})')\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "        # logging + logZ update\n",
        "        if t % print_every == 0:\n",
        "            with torch.no_grad():\n",
        "                s_chk  = sample_proposal(200_000)\n",
        "                logw_c = density.log_prob(s_chk) + math.log(box_area)\n",
        "\n",
        "                logw_clamped = torch.clamp(logw_c, min=-8.0, max=8.0)\n",
        "                w_c = torch.exp(logw_clamped)\n",
        "\n",
        "                # numerically stable estimate of E_m[w]\n",
        "\n",
        "                shift = logw_c.max()\n",
        "                w_mean = (torch.exp(logw_c - shift).mean() * torch.exp(shift)).item()\n",
        "\n",
        "                # logZ := logZ + τ * log(E_m[w]) so mass is nudged toward 1\n",
        "                delta_logZ = math.log(max(w_mean, 1e-6))\n",
        "                density.logZ.add_(logZ_tau * delta_logZ)\n",
        "\n",
        "\n",
        "            print(f\"Iteration {t}/{T}: L_est={L_val:.6f} | E_m[w]≈{w_mean:.3f}\", end='', flush=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                bowl_mask = s_chk[:, 0].abs() > 0.6\n",
        "                bridge_mask = s_chk[:, 0].abs() < 0.3\n",
        "                w_bowls = w_c[bowl_mask].mean().item() if bowl_mask.sum() > 0 else 0.0\n",
        "                w_bridge = w_c[bridge_mask].mean().item() if bridge_mask.sum() > 0 else 0.0\n",
        "                print(f\"\\n--- Weight Check (Iter {t}): \"\n",
        "                      f\"Avg Bowl Weight = {w_bowls:.4f} | Avg Bridge Weight = {w_bridge:.4f} ---\")\n",
        "                csv_writer.writerow([t, L_val, w_mean, w_bowls, w_bridge])\n",
        "                csv_file.flush()\n",
        "\n",
        "    csv_file.close()\n",
        "    print(f\"\\nTraining complete. Metrics saved to {csv_filename}\")\n",
        "    if video_frames:\n",
        "        render_state_distribution_video(\n",
        "            list(video_frames),\n",
        "            extent=plot_extent,\n",
        "            norm=plot_norm,\n",
        "            save_path=video_path\n",
        "        )\n",
        "    return hist, density"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save as make_videos.py and run: python make_videos.py\n",
        "import glob\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "def build_video(pattern, output, fps=5):\n",
        "    frames = sorted(glob.glob(pattern))\n",
        "    if not frames:\n",
        "        print(f\"No frames matching {pattern}\")\n",
        "        return\n",
        "    with imageio.get_writer(output, fps=fps) as writer:\n",
        "        for f in frames:\n",
        "            writer.append_data(imageio.imread(f))\n",
        "    print(f\"Wrote {output} ({len(frames)} frames)\")\n",
        "\n",
        "build_video(\"plots/state_distribution/state_dist_*.png\", \"state_distribution.mp4\")\n",
        "build_video(\"plots/policy/policy_*.png\", \"policy.mp4\")\n",
        "build_video(\"plots/value/value_*.png\", \"value.mp4\")"
      ],
      "metadata": {
        "id": "zY9VacK0VrNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd18a3b0-b6dc-4a71-cf4c-f4db10681e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 600) to (800, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 600) to (800, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote state_distribution.mp4 (200 frames)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 600) to (800, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote policy.mp4 (200 frames)\n",
            "Wrote value.mp4 (200 frames)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1WfSHGWXi29r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}