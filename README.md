---

# ðŸ§  Soft Actor-Critic (SAC) on MountainCarContinuous-v0

This project implements the **Soft Actor-Critic (SAC)** reinforcement learning algorithm on the classic `MountainCarContinuous-v0` environment using [Gymnasium](https://gymnasium.farama.org/). The codebase is written in **PyTorch** and includes full training, evaluation, and logging functionality via **TensorBoard**.

---

## ðŸš— Problem Description

The **Mountain Car Continuous** environment involves driving a car up a steep hill. The carâ€™s engine alone is not powerful enough to climb the hill directly, so the agent must learn to build momentum by moving back and forth.

* **Observation space**: Continuous `[position, velocity]`
* **Action space**: Continuous throttle force
* **Reward**: +100 for reaching the goal, otherwise -1 per timestep
* **Max steps per episode**: 2000

---

## ðŸ¤– Algorithm: Soft Actor-Critic (SAC)

SAC is an off-policy actor-critic algorithm based on the **maximum entropy framework**, encouraging both reward maximization and exploration.

### ðŸ”‘ Key Features

* **Stochastic policy** for better exploration
* **Twin Q-networks** to address overestimation bias
* **Automatic entropy tuning** for stability
* **Replay buffer** and **target networks**
* **Gradient updates per step**, customizable

---

## ðŸ—ï¸ Project Structure

```plaintext
.
â”œâ”€â”€ train.py                # Main training loop
â”œâ”€â”€ sac.py                  # SAC algorithm class
â”œâ”€â”€ network.py              # Policy and Q-value networks
â”œâ”€â”€ buffer.py               # Experience replay buffer
â”œâ”€â”€ utils.py                # Helper functions for plotting and updates
â”œâ”€â”€ reward_wrapper.py       # Custom reward reshaping
â”œâ”€â”€ checkpoints/            # Model checkpoints
â”œâ”€â”€ plots/                  # Learning curve plots
â”œâ”€â”€ runs/                   # TensorBoard logs
```

---

## ðŸ§ª Training Details

| Hyperparameter            | Value      |
| ------------------------- | ---------- |
| Optimizer                 | Adam       |
| Learning Rate             | 0.0003     |
| Discount Factor (Î³)       | 0.999      |
| Soft Target Update (Ï„)    | 0.005      |
| Batch Size                | 256        |
| Replay Buffer Size        | 100,000    |
| Start Steps (Exploration) | 10,000     |
| Policy Type               | Gaussian   |
| Updates per Step          | 1          |
| Gradient Steps            | 8          |
| Entropy Coefficient (Î±)   | Auto-tuned |
| Max Episode Steps         | 2000       |

---

## ðŸ“ˆ Results

During training, we log key metrics including:

* Actor & Critic losses
* Entropy temperature
* Episode rewards
* Evaluation rewards

Final performance is visualized in the learning curve and gameplay video.

### ðŸŽ¥ Demo Video (SAC Agent Solving the Task)

> ðŸš© The agent successfully reaches the flag by learning to build momentum!

[![Watch Video](https://img.youtube.com/vi_webhook/0.jpg)](https://github.com/Micro046/SAC_Mountain_Car_Problem/blob/main/mountaincar_sac.mp4)
ðŸ‘‰ [View Video on GitHub](https://github.com/Micro046/SAC_Mountain_Car_Problem/blob/main/mountaincar_sac.mp4)


## ðŸ“š References

* Haarnoja et al. (2018). *Soft Actor-Critic Algorithms for Deep Reinforcement Learning*
* OpenAI Gymnasium Docs: [MountainCarContinuous-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car/)
* PyTorch Documentation: [pytorch.org](https://pytorch.org)

---
